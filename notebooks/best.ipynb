{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hybrid-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "radio-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "args = argparse.Namespace()\n",
    "\n",
    "args.n_splits = 5\n",
    "args.n_jobs = -1\n",
    "args.val_fraction = 0.1\n",
    "args.epochs = 50\n",
    "args.seed=101\n",
    "args.reproducibility = True\n",
    "\n",
    "args.raw_train = pd.read_csv('../data/train.csv.zip')\n",
    "args.raw_test = pd.read_csv('../data/test.csv.zip')\n",
    "args.train = args.raw_train.iloc[:, 1:].copy()\n",
    "args.labels = args.raw_train['label'].copy()\n",
    "args.test = args.raw_test.copy()\n",
    "args.predictions_folder = Path('../predictions/best')\n",
    "args.models_folder = Path('../models/best')\n",
    "\n",
    "args.model_name = 'best.hdf5'\n",
    "\n",
    "args.predictions_folder.mkdir(parents=True, exist_ok=True) \n",
    "args.models_folder.mkdir(parents=True, exist_ok=True) \n",
    "args.run_baseline = 0\n",
    "args.run_transfer = 1\n",
    "args.run_augment = 0\n",
    "args.run_ensemble = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dress-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def reproducibility():\n",
    "    if args.reproducibility:\n",
    "        np.random.seed(args.seed)\n",
    "        tf.random.set_seed(args.seed)\n",
    "\n",
    "reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brave-april",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 36x36 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "def draw_digit(pixels_2d, label=None, size_inches=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    if label is not None:\n",
    "        ax.set_title(label)\n",
    "    if size_inches:\n",
    "        fig.set_size_inches(size_inches[0], size_inches[1])\n",
    "    imgplot = ax.imshow(pixels_2d, cmap='gray')\n",
    "\n",
    "random_row = random.randrange(0, args.raw_train.shape[0], 1)\n",
    "label = args.raw_train.iloc[random_row, 0]\n",
    "pixels_2d = args.raw_train.iloc[random_row, 1:].to_numpy().reshape(28, 28)\n",
    "draw_digit(pixels_2d, label, (0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fixed-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:08:28,987 : INFO : X_train_full.shape: (42000, 28, 28, 1)\n",
      "2021-05-25 12:08:28,988 : INFO : Y_train_full.shape: (42000, 10)\n",
      "2021-05-25 12:08:28,993 : INFO : Type of target Y_train_full: 'multilabel-indicator'\n",
      "2021-05-25 12:08:28,994 : INFO : y_train_full.shape: (42000,)\n",
      "2021-05-25 12:08:28,995 : INFO : Type of target y_train_full: 'multiclass'\n"
     ]
    }
   ],
   "source": [
    "import sklearn.utils.multiclass\n",
    "\n",
    "X_train_full = args.train.to_numpy().reshape(args.train.shape[0], 28, 28, 1) / 255.0\n",
    "Y_train_full = pd.get_dummies(args.labels, prefix='label').to_numpy()\n",
    "y_train_full = args.labels.to_numpy()\n",
    "X_test = args.test.to_numpy().reshape(args.test.shape[0], 28, 28, 1) / 255.0\n",
    "\n",
    "log.info('X_train_full.shape: %s', repr(X_train_full.shape))\n",
    "# log.info('X[0][14][14]: %s', X[0][14][14])\n",
    "\n",
    "log.info('Y_train_full.shape: %s', repr(Y_train_full.shape))\n",
    "# log.info('y[0], %s', y[0])\n",
    "log.info('Type of target Y_train_full: %s', repr(sklearn.utils.multiclass.type_of_target(Y_train_full)))\n",
    "\n",
    "log.info('y_train_full.shape: %s', repr(y_train_full.shape))\n",
    "# log.info('y_sparse: %s', repr(y_sparse))\n",
    "# log.info('y_sparse[0]: %s', y_sparse[0])\n",
    "log.info('Type of target y_train_full: %s', repr(sklearn.utils.multiclass.type_of_target(y_train_full)))\n",
    "\n",
    "def plot_history(history):\n",
    "    log.info(\"History keys: %s\", history.history.keys())\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(history.history['accuracy'], label='Train')\n",
    "    ax.plot(history.history['val_accuracy'], label='Test')\n",
    "    ax.set_title('Model accuracy')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.grid(True)\n",
    "    ax.legend(['Train', 'Val'], loc='lower right')\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def csv_predictions(predictions, filename):\n",
    "    log.debug('predictions.shape: %s', repr(predictions.shape))\n",
    "    predictions_classes = np.argmax(predictions, axis=1)\n",
    "    csv_predictions_categorical(predictions_classes, filename)\n",
    "\n",
    "def csv_predictions_categorical(predictions_classes, filename):\n",
    "    image_ids = np.arange(1, len(predictions_classes) + 1)\n",
    "    submission = pd.DataFrame({'ImageId': image_ids, 'Label': predictions_classes})\n",
    "    filepath = args.predictions_folder/filename\n",
    "    submission.to_csv(filepath, index=False)\n",
    "    log.info('Saved file: %s', filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-necklace",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tribal-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def baseline_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(28, 28, 1)))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=10, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=3, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    return model, [early_stopping, reduce_lr_on_plateau]\n",
    "\n",
    "def baseline():\n",
    "    reproducibility()\n",
    "    model, callbacks = baseline_model()\n",
    "    \n",
    "    history = model.fit(X_train_full, y_train_full, validation_split=args.val_fraction, epochs=args.epochs, \n",
    "                        callbacks=callbacks, verbose=1)\n",
    "    plot_history(history)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    csv_predictions(predictions, 'baseline.csv')\n",
    "\n",
    "if args.run_baseline:\n",
    "    baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-place",
   "metadata": {},
   "source": [
    "| Model | Params  | Test score  | CV mean score |\n",
    "|---|---|---|---|\n",
    "| Baseline | No dropout, early_stopping='val_acc' | 0.99017 | 0.9924 |\n",
    "| Baseline | Dropout=0.4, early_stopping='val_acc' | 0.99200 | 0.9931 |\n",
    "| Baseline | Dropout=0.4, Batch normalization, early_stopping='val_acc'  | 0.99278 |  0.9943 |\n",
    "| Baseline | Dropout=0.4, Batch normalization, early_stopping='val_loss'  | 0.99389 |  0.9948 |\n",
    "\n",
    "\n",
    "##### Other results\n",
    "| Model | Params  | Test score  | CV mean score |\n",
    "|---|---|---|---|\n",
    "| Baseline | Dropout=0.25 (after 2 first conv) | 0.99150 | 0.9929 |\n",
    "| Baseline | Dropout=0.5 (after 2 first conv) | 0.99178 | 0.9924 |\n",
    "| Baseline | Dropout=0.5  | 0.98867 | 0.9912 |\n",
    "| Baseline | Dropout=0.3  | 0.99182 |  0.9936 |\n",
    "| Baseline | Dropout=0.4 batch normalization before activation  | 0.99275 |  0.9929 |\n",
    "| Baseline | Dropout=0.4 batch normalization he normal | 0.99250 |  0.9940 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-texture",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "endless-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'block1_conv2_bn/gamma:0' shape=(64,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'block1_conv2_bn/beta:0' shape=(64,) dtype=float32, numpy=\n",
      "array([ 1.02501655e+00, -1.29600570e-01,  1.42564714e-01, -2.75671810e-01,\n",
      "       -1.99757144e-01,  1.77486725e-02,  7.30893314e-01,  1.27452150e-01,\n",
      "       -1.64462030e+00,  1.81503192e-01,  1.05904363e-01,  9.22831222e-02,\n",
      "        2.21073315e-01,  3.24689388e-01,  1.70133102e+00,  4.57326678e-04,\n",
      "       -5.09546027e-02,  4.23189372e-01,  7.11034060e+00,  2.22172308e+00,\n",
      "        2.12837672e+00,  6.10172078e-02, -3.66390556e-01, -8.53206456e-01,\n",
      "       -7.75102079e-01,  3.11801195e-01,  1.52496517e+00, -2.23411560e+00,\n",
      "       -5.29502332e-01,  1.51518989e+00,  6.38463557e-01, -2.12757874e+00,\n",
      "        1.26488948e+00,  2.18932152e+00, -1.84421134e+00, -1.62967288e+00,\n",
      "        2.01567149e+00,  1.45450979e-01,  2.11188579e+00,  6.68150663e-01,\n",
      "        5.88038146e-01,  1.05498925e-01, -1.46761036e+00, -5.55007219e-01,\n",
      "       -3.84303302e-01, -2.03040928e-01,  3.23828399e-01, -3.22847635e-01,\n",
      "       -1.01928878e+00,  1.72932124e+00,  1.77079284e+00,  6.14907801e-01,\n",
      "       -1.56757355e+00,  2.24159336e+00, -1.97178531e+00, -1.42489684e+00,\n",
      "       -1.64092326e+00,  8.67824614e-01,  1.85728633e+00,  8.43130708e-01,\n",
      "        1.42433763e+00,  8.07423353e-01, -8.55507562e-04,  2.64977142e-02],\n",
      "      dtype=float32)>, <tf.Variable 'block1_conv2_bn/moving_mean:0' shape=(64,) dtype=float32, numpy=\n",
      "array([ 6.7334872e-01, -5.1775485e-01,  2.2719330e-01, -2.4155145e+00,\n",
      "       -1.8364482e+00, -1.2555783e+00, -4.2245922e+00,  4.2802033e+00,\n",
      "        4.0435309e+00, -3.4525754e+00, -1.1096232e+00,  4.0445147e+00,\n",
      "       -3.6829019e+00,  5.4934034e+00,  1.4265063e+00, -2.7156065e+00,\n",
      "       -2.7161648e+00, -2.3781283e+00, -5.1020985e+00,  2.7429562e+00,\n",
      "       -4.1370535e+00, -2.1964417e+00, -3.8226491e-01,  5.4728413e-01,\n",
      "        2.0296791e+00,  3.7991605e+00, -2.8589993e+00,  7.7404189e-03,\n",
      "       -1.8760828e+00, -5.2108288e+00, -9.9376535e+00,  5.6162369e-03,\n",
      "       -7.6995200e-01, -6.4646950e+00, -1.5195684e+00,  1.0987047e+00,\n",
      "        2.5020499e+00,  7.3626270e+00, -1.3528596e+00,  2.9162281e+00,\n",
      "       -3.9019077e+00,  2.0467813e+00, -2.7018493e-01,  2.9194672e+00,\n",
      "       -2.6759923e+00, -1.5455512e+00, -6.4347758e+00, -1.5217922e+00,\n",
      "       -4.0690511e-01, -1.4546899e+00,  1.6080253e+00,  4.9044752e+00,\n",
      "       -1.1409745e+00,  2.2657065e+00, -2.1499510e+00, -1.8886544e+00,\n",
      "       -1.0836674e+00,  3.6654286e+00, -4.4338498e-01,  4.4831328e+00,\n",
      "       -2.0363557e+00,  2.0740986e+00,  1.6211330e+00,  2.1356399e+00],\n",
      "      dtype=float32)>, <tf.Variable 'block1_conv2_bn/moving_variance:0' shape=(64,) dtype=float32, numpy=\n",
      "array([3.39573240e+00, 3.74108267e+00, 3.17113090e+00, 2.36947727e+00,\n",
      "       8.20735836e+00, 3.63453746e+00, 3.78069115e+00, 7.84879732e+00,\n",
      "       7.57964253e-01, 7.20808554e+00, 3.73265481e+00, 2.82348156e+00,\n",
      "       1.79905784e+00, 5.95087280e+01, 1.00864744e+01, 3.98722935e+00,\n",
      "       5.33041096e+00, 4.35808029e+01, 1.71469669e+01, 9.76100063e+00,\n",
      "       8.45604420e+00, 3.87665296e+00, 8.69978333e+00, 2.43925905e+00,\n",
      "       4.90698767e+00, 6.80966797e+01, 2.20571923e+00, 2.62107387e-05,\n",
      "       5.47730303e+00, 1.84390390e+00, 1.19040813e+01, 8.88918585e-05,\n",
      "       2.13850212e+00, 1.75237808e+01, 6.89077079e-01, 5.93119681e-01,\n",
      "       1.62066860e+01, 2.90466714e+00, 1.17835455e+01, 3.46346688e+00,\n",
      "       8.46537876e+00, 2.07618618e+01, 6.19788766e-01, 4.54517508e+00,\n",
      "       3.47181582e+00, 8.48945999e+00, 5.28503265e+01, 5.75978947e+00,\n",
      "       4.75143385e+00, 1.96984899e+00, 1.94297850e+00, 4.19577789e+00,\n",
      "       7.71195114e-01, 8.68150997e+00, 5.65264761e-01, 5.42122483e-01,\n",
      "       4.12049103e+00, 3.99619412e+00, 6.55178642e+00, 3.50689430e+01,\n",
      "       2.48166299e+00, 1.93062830e+00, 2.91381025e+00, 2.92604446e+00],\n",
      "      dtype=float32)>]\n",
      "Start\n",
      "(3, 3, 3, 32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "slice index 1000 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a7af0f1113ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_transfer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtransfer_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a7af0f1113ec>\u001b[0m in \u001b[0;36mtransfer_learning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-a7af0f1113ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(base_model, model, train_data, val_data, train_base, epochs, lr)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_SliceHelperVar\u001b[0;34m(var, slice_spec)\u001b[0m\n\u001b[1;32m   1291\u001b[0m   \"\"\"\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_slice_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1034\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m     return strided_slice(\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[1;32m   1210\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10445\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10446\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10447\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10448\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10449\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/digit-recongizer/.venv/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: slice index 1000 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def transfer_learning():\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "#     train_idx = len(X_train_full) - int(len(X_train_full) * args.val_fraction)\n",
    "#     val_idx = train_idx\n",
    "    \n",
    "    train_idx = 50\n",
    "    val_idx = len(X_train_full) - train_idx \n",
    "    def preprocess(image, label):\n",
    "        resized_image = tf.image.resize(image, [224, 224])\n",
    "        rgb_image = tf.image.grayscale_to_rgb(resized_image)\n",
    "        final_image = keras.applications.xception.preprocess_input(rgb_image)\n",
    "        return final_image, label\n",
    "    \n",
    "    def dataset(features_labels):\n",
    "        def debug_data(data):\n",
    "            for d in data:\n",
    "                print(d)\n",
    "                fig, ax = plt.subplots()\n",
    "                imgplot = ax.imshow(d[0][0])\n",
    "                break\n",
    "        data = tf.data.Dataset.from_tensor_slices(features_labels).shuffle(len(features_labels[0])).map(preprocess).batch(batch_size).prefetch(1)\n",
    "        return data\n",
    "    \n",
    "    def build_model():\n",
    "        base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "        print(base_model.layers[5].weights)\n",
    "        avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "        output = keras.layers.Dense(10, activation='softmax')(avg)\n",
    "        model = keras.Model(inputs=base_model.input, outputs=output)\n",
    "        return base_model, model\n",
    "    \n",
    "    \n",
    "    def train(base_model, model, train_data, val_data, train_base=False, epochs=epochs, lr=0.2):\n",
    "        print('Start')\n",
    "        print(base_model.weights[0].shape)\n",
    "        print(model.weights[0][1000][0])\n",
    "        for l in base_model.layers:\n",
    "            l.trainable = train_base\n",
    "        optimizer = keras.optimizers.SGD(lr=lr, momentum=0.9, decay=0.01)\n",
    "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        history = model.fit(train_data, epochs=epochs, validation_data=val_data)\n",
    "        plot_history(history)\n",
    "        print('End')\n",
    "        print(model.weights[100][0][0])\n",
    "    \n",
    "    train_data = dataset((X_train_full[:train_idx], y_train_full[:train_idx]))\n",
    "    val_data = dataset((X_train_full[val_idx:], y_train_full[val_idx:]))\n",
    "    base_model, model = build_model()\n",
    "    train(base_model, model, train_data, val_data)\n",
    "    train(base_model, model, train_data, val_data, train_base=True, lr=0.01)\n",
    "\n",
    "if args.run_transfer:\n",
    "    transfer_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-knowing",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_augmentator():\n",
    "        datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        #   shear_range=0.05,\n",
    "        #   channel_shift_range=0.05,\n",
    "            rotation_range=5,\n",
    "            zoom_range=0.05,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05)\n",
    "        return datagen\n",
    "\n",
    "def data_augmentation():\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=args.val_fraction, \n",
    "                                                      random_state=args.seed)\n",
    "    datagen = data_augmentator()\n",
    "#     val_modified_batch = datagen.flow(X_val, y_val, seed=args.seed).next()\n",
    "#     draw_digit(val_modified_batch[0][0], val_modified_batch[1][0])\n",
    "    \n",
    "    datagen.fit(X_train, seed=args.seed)\n",
    "    train_generator = datagen.flow(X_train, y_train, seed=args.seed)\n",
    "    model, callbacks = baseline_model()\n",
    "    history = model.fit(train_generator, epochs=args.epochs, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "    plot_history(history)\n",
    "    predictions = model.predict(X_test)\n",
    "    csv_predictions(predictions, 'augment.csv')\n",
    "\n",
    "if args.run_augment:\n",
    "    data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-cause",
   "metadata": {},
   "source": [
    "| Model | Params  | Test score  | CV mean score | Val loss |\n",
    "|---|---|---|---|---|\n",
    "| Augment run 1 | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1  | 0.99421 | 0.9936 | 0.0188|\n",
    "| Augment run 2 | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1  | 0.99453 | 0.9964 | 0.0135|\n",
    "| Augment run 3 | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1  | 0.99435 | 0.9948 | 0.0151|\n",
    "| Augment | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=5, zoom_range=0.05, width_shift_range=0.05, height_shift_range=0.05  | 0.99503 | 0.9950 | 0.0147 |\n",
    "| Augment | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=2, zoom_range=0.02, width_shift_range=0.02, height_shift_range=0.02  | 0.99392 | 0.9962 | 0.0160 |\n",
    "| Augment | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=7, zoom_range=0.07, width_shift_range=0.07, height_shift_range=0.07  | 0.99453 | 0.9950 | 0.0144 |\n",
    "| Augment | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=5, zoom_range=0.05, width_shift_range=0.05, height_shift_range=0.05  | 0.99503 | 0.9950 | 0.0147 |\n",
    "| Augment | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=5, zoom_range=0.05, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.05, channel_shift_range=0.05  | 0.99475 | 0.9950 | 0.0139 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-calvin",
   "metadata": {},
   "source": [
    "### Ensemble + data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def ensemble(num_models=5):\n",
    "    models = []\n",
    "    for model_num in range(num_models):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=args.val_fraction, \n",
    "                                                  random_state=model_num)\n",
    "        datagen = data_augmentator()\n",
    "        datagen.fit(X_train, seed=model_num)\n",
    "        train_generator = datagen.flow(X_train, y_train, seed=args.seed)\n",
    "        model, callbacks = baseline_model()\n",
    "        history = model.fit(train_generator, epochs=args.epochs, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "        plot_history(history)\n",
    "        models.append(model)\n",
    "    \n",
    "    predictions_classes = []\n",
    "    for model in models:\n",
    "        predictions_cur = model.predict(X_test)\n",
    "        predictions_classes_cur = np.argmax(predictions_cur, axis=1)\n",
    "        predictions_classes.append(predictions_classes_cur)\n",
    "    \n",
    "    def bag_predictions(predictions_classes):\n",
    "        predictions_classes_bagged = []\n",
    "        predictions_classes_stacked = np.stack(predictions_classes, axis=-1)\n",
    "        for prediction_i in predictions_classes_stacked:\n",
    "            predictions_classes_bagged.append(Counter(prediction_i).most_common(1)[0][0])\n",
    "        return predictions_classes_bagged\n",
    "    \n",
    "    csv_predictions = csv_predictions_categorical(bag_predictions(predictions_classes), 'ensemble-augment.csv')\n",
    "\n",
    "if args.run_ensemble:\n",
    "    ensemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-prince",
   "metadata": {},
   "source": [
    "| Model | Params  | Test score  | CV mean score | Val loss |\n",
    "|---|---|---|---|---|\n",
    "| Augment ensemble 5 models | Baseline model, Dropout=0.4, Batch normalization, early_stopping='val_loss', rotation_range=5, zoom_range=0.05, width_shift_range=0.05, height_shift_range=0.05  | 0.99517 | - | - |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
